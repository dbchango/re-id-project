{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing models with Keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 96)          34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 96)          384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 384)         885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 7007      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 7)                 28        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 25,727,491\n",
      "Trainable params: 25,706,341\n",
      "Non-trainable params: 21,150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "AlexNet = Sequential()\n",
    "\n",
    "# 1 CONV LAYER\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32, 32, 3), kernel_size=(11, 11), strides=(4, 4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# 2 CONV LAYER\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# 3 CONV LAYER\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 4 CONV LAYER\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 5 CONV LAYER\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# FC LAYERS\n",
    "AlexNet.add(Flatten())\n",
    "# 1st FC LAYER\n",
    "AlexNet.add(Dense(4096,input_shape=(32, 32, 3, )))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# DROP OUT\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 2nd FC LAYER\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# ADD DROPOUT\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 3rd FC LAYER\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# ADD DROPOUT\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# OUTPUT LAYER\n",
    "AlexNet.add(Dense(7))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "# Model Summary\n",
    "AlexNet.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "AlexNet.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# lets count data\n",
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "train_data = 'Datasets/Propio/primer_grupo/train_data'\n",
    "test_data = 'Datasets/Propio/primer_grupo/test_data'\n",
    "validation_data = 'Datasets/Propio/primer_grupo/validation_data'\n",
    "\n",
    "train_ds = keras.preprocessing.image\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 489 images belonging to 7 classes.\n",
      "Found 105 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "im_width, im_height = 32, 32\n",
    "nb_train_samples = 8144\n",
    "nb_validation_samples = 8041\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "n_classes = 7\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=5,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "train_gen = datagen.flow_from_directory(train_data, target_size=(im_width, im_height), class_mode='categorical', batch_size=batch_size)\n",
    "validation_gen = datagen.flow_from_directory(validation_data, target_size=(im_width, im_height), class_mode='categorical', batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\n",
    "callbacks_list = [early_stop, reduce_lr]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 45s 3s/step - loss: 1.5242 - acc: 0.4442 - val_loss: 13.8148 - val_acc: 0.1429\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 35s 2s/step - loss: 0.9700 - acc: 0.7105 - val_loss: 13.2471 - val_acc: 0.1429\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 34s 2s/step - loss: 0.8053 - acc: 0.8073 - val_loss: 7.7570 - val_acc: 0.2567\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 34s 2s/step - loss: 0.6936 - acc: 0.8261 - val_loss: 2.9694 - val_acc: 0.2678\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 35s 2s/step - loss: 0.6266 - acc: 0.8574 - val_loss: 2.5448 - val_acc: 0.2834\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 37s 2s/step - loss: 0.5727 - acc: 0.8997 - val_loss: 2.9139 - val_acc: 0.2870\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 38s 2s/step - loss: 0.5250 - acc: 0.9150 - val_loss: 5.9642 - val_acc: 0.1426\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 37s 2s/step - loss: 0.5242 - acc: 0.8982 - val_loss: 5.3002 - val_acc: 0.1431\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 38s 2s/step - loss: 0.4807 - acc: 0.9268 - val_loss: 7.4169 - val_acc: 0.1427\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 39s 2s/step - loss: 0.4621 - acc: 0.9534 - val_loss: 6.0393 - val_acc: 0.1429\n"
     ]
    }
   ],
   "source": [
    "model_history = AlexNet.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=nb_validation_samples//batch_size,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(0)\n",
    "plt.plot(model_history.history['acc'], 'r')\n",
    "plt.plot(model_history.history['val_acc'], 'g')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train', 'validation'])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(model_history.history['loss'], 'r')\n",
    "plt.plot(model_history.history['val_loss'], 'g')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train', 'validation'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AlexNet.evaluate_generator(validation_gen, steps=None, max_queue_size=10, workers=1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = AlexNet.predict_generator(validation_gen, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "predicted = np.argmax(pred, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Python37\\lib\\site-packages\\pixellib\\instance\\mask_rcnn.py:409: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Python37\\lib\\site-packages\\pixellib\\instance\\mask_rcnn.py:431: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.ReID import extract_masks, apply_mask, dpm, lbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001__8 01.jpg\n",
      "001__8 02.jpg\n",
      "001__8 03.jpg\n",
      "001__8 04.jpg\n",
      "001__8 05.jpg\n",
      "001__8 06.jpg\n",
      "001__8 07.jpg\n",
      "001__8 08.jpg\n",
      "001__8 09.jpg\n",
      "001__8 10.jpg\n",
      "001__8 11.jpg\n",
      "001__8 12.jpg\n",
      "001__8 13.jpg\n",
      "001__8 14.jpg\n",
      "001__8 15.jpg\n",
      "001__8 16.jpg\n",
      "001__8 17.jpg\n",
      "001__8 18.jpg\n",
      "001__8 19.jpg\n",
      "001__8 20.jpg\n",
      "001__8 21.jpg\n",
      "001__8 22.jpg\n",
      "001__8 23.jpg\n",
      "001__8 24.jpg\n",
      "001__8 25.jpg\n",
      "001__8 26.jpg\n",
      "001__8 27.jpg\n",
      "001__8 28.jpg\n",
      "001__8 29.jpg\n",
      "001__8 30.jpg\n",
      "001__8 31.jpg\n",
      "001__8 32.jpg\n",
      "001__8 33.jpg\n",
      "001__8 34.jpg\n",
      "001__8 35.jpg\n",
      "001__8 36.jpg\n",
      "001__8 37.jpg\n",
      "001__8 38.jpg\n",
      "001__8 39.jpg\n",
      "001__8 40.jpg\n",
      "001__8 41.jpg\n",
      "001__8 42.jpg\n",
      "001__8 43.jpg\n",
      "001__8 44.jpg\n",
      "001__8 45.jpg\n",
      "001__8 46.jpg\n",
      "001__8 47.jpg\n",
      "001__8 48.jpg\n",
      "001__8 49.jpg\n",
      "001__8 50.jpg\n",
      "001__8 51.jpg\n"
     ]
    }
   ],
   "source": [
    "path = 'Datasets/Propio/1/cuarto'\n",
    "for image_path in os.listdir(path):\n",
    "    root = os.path.join(path, image_path)\n",
    "    frame = cv2.imread(root)\n",
    "    scale = 10\n",
    "    width = int(frame.shape[1] * scale / 100)\n",
    "    height = int(frame.shape[0] * scale / 100)\n",
    "    frame = cv2.resize(frame, (width, height))\n",
    "    frame_cp = frame.copy()\n",
    "    r, output = extract_masks(frame)\n",
    "    if len(r[\"rois\"]) != 0 and len(r[\"masks\"]) != 0:\n",
    "        for i in range(len(r[\"rois\"])):\n",
    "            mask = r[\"masks\"][:, :, i].astype(int)\n",
    "            temp_frame = apply_mask(frame_cp, mask)\n",
    "            h_crop, t_crop, l_crop = dpm(r[\"rois\"][i], temp_frame)\n",
    "            head_hg = lbp(h_crop)\n",
    "            torso_hg = lbp(t_crop)\n",
    "            legs_hg = lbp(l_crop)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from utils.ReID import extract_masks, calc_lbph, dpm, apply_mask, crop_frame, lbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  Datasets/Propio/primer_grupo/test_data\\1\n",
      "Processing person_1_1 ...\n",
      "Opening  Datasets/Propio/primer_grupo/test_data\\1\\001_9 16.jpg\n",
      "x1: 94, y1: 149, x2: 703, y2: 338\n",
      "Processing person_1_2 ...\n",
      "Opening  Datasets/Propio/primer_grupo/test_data\\1\\001_9 17.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8744\\3262460061.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCOLOR_BGR2RGB\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mimage_cp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_masks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'masks'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'rois'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"rois\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\GitHub\\re-id-project\\utils\\ReID.py\u001B[0m in \u001B[0;36mextract_masks\u001B[1;34m(frame)\u001B[0m\n\u001B[0;32m     31\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[1;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprocessed_image\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m     \"\"\"\n\u001B[1;32m---> 33\u001B[1;33m     \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mseg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msegment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[1;31m# TODO: add area calculation function (@PAMELA), this function will calculate tha area of\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\GitHub\\re-id-project\\utils\\MaskRCNN.py\u001B[0m in \u001B[0;36msegment\u001B[1;34m(self, frame)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0msegment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m         \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msegmentFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msegment_target_classes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_classes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\pixellib\\instance\\__init__.py\u001B[0m in \u001B[0;36msegmentFrame\u001B[1;34m(self, frame, show_bboxes, segment_target_classes, extract_segmented_objects, text_thickness, text_size, box_thickness, save_extracted_objects, mask_points_values, output_image_name, verbose)\u001B[0m\n\u001B[0;32m    440\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mverbose\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Processing image...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 442\u001B[1;33m         \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnew_frame\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    443\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\pixellib\\instance\\mask_rcnn.py\u001B[0m in \u001B[0;36mdetect\u001B[1;34m(self, images, verbose)\u001B[0m\n\u001B[0;32m   2462\u001B[0m         \u001B[1;31m# Run object detection\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2463\u001B[0m         \u001B[0mdetections\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmrcnn_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2464\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mmolded_images\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_metas\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0manchors\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2465\u001B[0m         \u001B[1;31m# Process detections\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2466\u001B[0m         \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m    906\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    907\u001B[0m         \u001B[0mworkers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 908\u001B[1;33m         use_multiprocessing=use_multiprocessing)\n\u001B[0m\u001B[0;32m    909\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    910\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mreset_metrics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \u001B[0msteps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 723\u001B[1;33m         callbacks=callbacks)\n\u001B[0m",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001B[0m in \u001B[0;36mmodel_iteration\u001B[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001B[0m\n\u001B[0;32m    392\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    393\u001B[0m         \u001B[1;31m# Get outputs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 394\u001B[1;33m         \u001B[0mbatch_outs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mins_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    395\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    396\u001B[0m           \u001B[0mbatch_outs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   3474\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3475\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[1;32m-> 3476\u001B[1;33m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[0;32m   3477\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3478\u001B[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001B[1;32mC:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1470\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[0;32m   1471\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1472\u001B[1;33m                                                run_metadata_ptr)\n\u001B[0m\u001B[0;32m   1473\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1474\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "parent_root = 'Datasets/Propio/primer_grupo/test_data'\n",
    "\n",
    "for person in os.listdir(parent_root):\n",
    "    person_root = os.path.join(parent_root, person)\n",
    "    print(\"Working on: \", person_root)\n",
    "    counter = 0\n",
    "    for instance in os.listdir(person_root):\n",
    "        counter += 1\n",
    "        name = 'person_{}_{}'.format(person, counter)\n",
    "        print('Processing {} ...'.format(name))\n",
    "        source_path = os.path.join(person_root, instance)\n",
    "        print(\"Opening \", source_path)\n",
    "        image = cv2.imread(source_path, cv2.COLOR_BGR2RGB)\n",
    "        image_cp = image.copy()\n",
    "        r, _ = extract_masks(image)\n",
    "        if len(r['masks']) != 0 and len(r['rois']) != 0:\n",
    "            for i in range(len(r[\"rois\"])):\n",
    "                mask = r[\"masks\"][:, :, i].astype(int)\n",
    "                masked_image = apply_mask(image_cp, mask)\n",
    "                x1, y1 = r[\"rois\"][i][0], r[\"rois\"][i][1]\n",
    "                x2, y2 = r[\"rois\"][i][2], r[\"rois\"][i][3]\n",
    "                print('x1: {}, y1: {}, x2: {}, y2: {}'.format(x1, y1, x2\n",
    "                                                              , y2))\n",
    "                cropped_frame = crop_frame(x1, x2, y1, y2, masked_image)\n",
    "                cropped_frame = lbp(cropped_frame)\n",
    "                cv2.imshow(\"\", cropped_frame)\n",
    "                cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}